{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecddcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper Functions\n",
    "\"\"\"\n",
    "\n",
    "# function checks if directory exists, if not it constructs it\n",
    "def check_directory_exists(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "# function saves DataFrame, list, or set as a textfile in a specific folder\n",
    "def save_to_text_file(output_folder_dest, input_data, text_file_name):\n",
    "    text_file_ouput = output_folder_dest + text_file_name + \".txt\"\n",
    "    drug_output_info_file = open(text_file_ouput, 'w+')\n",
    "    if isinstance(input_data, pd.DataFrame):\n",
    "        drug_output_info_file.write(input_data.to_string())\n",
    "    else:\n",
    "        drug_output_info_file.write(str(input_data))\n",
    "    drug_output_info_file.close() \n",
    "    print(\"Constructed and saved\", text_file_ouput)\n",
    "\n",
    "# Read in Pickle File\n",
    "def read_pickle_file(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        sys.exit(\"Can't locate input file %s\" % file_path)\n",
    "    return pd.read_pickle(file_path)\n",
    "    \n",
    "# Save data into a pickel file\n",
    "def save_to_pickle_file(output_folder_dest, dict_data, dict_file_name):\n",
    "    output_dict_filename = output_folder_dest + dict_file_name + '.pkl'\n",
    "    with open(output_dict_filename, 'wb') as handle:\n",
    "        pickle.dump(dict_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Constructed and saved\", output_dict_filename)\n",
    "\n",
    "# Read in a CSV file\n",
    "def read_csv_file(file_path, input_sep=',', input_delimiter=None, input_index_col=None, input_dtype=None, input_delim_whitespace=False, input_low_memory=True):\n",
    "    if not os.path.exists(file_path):\n",
    "        sys.exit(\"Can't locate input file %s\" % file_path)\n",
    "    return pd.read_csv(filepath_or_buffer=file_path, sep=input_sep, delimiter=None, index_col=input_index_col, dtype=input_dtype, delim_whitespace=input_delim_whitespace, low_memory=input_low_memory)\n",
    "\n",
    "# funcion saves DataFrame or list to as a textfile\n",
    "def save_to_csv_file(output_folder_dest, df, csv_file_name, input_index=False):\n",
    "    output_filename = output_folder_dest + csv_file_name + \".csv\"\n",
    "    df.to_csv(output_filename, index=input_index)\n",
    "    print(\"Constructed and saved\", output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d8399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = \"9606.protein.links.full.v10.5.txt\"\n",
    "\n",
    "if not os.path.exists(input_filename):\n",
    "    sys.exit(\"Can't locate input file %s\" % input_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a90800",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = 0.041"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a53c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prior_away(score, prior):\n",
    "\n",
    "    if score < prior: score = prior\n",
    "    score_no_prior = (score - prior) / (1 - prior)\n",
    "\n",
    "    return score_no_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = True\n",
    "for line in open(input_filename):\n",
    "\n",
    "    if header:\n",
    "        header = False\n",
    "        continue\n",
    "    \n",
    "    l = line.split()\n",
    "    \n",
    "    ## load the line\n",
    "        \n",
    "    (protein1, protein2,\n",
    "     neighborhood, neighborhood_transferred,\n",
    "     fusion, cooccurrence,\n",
    "     homology,\n",
    "     coexpression, coexpression_transferred,\n",
    "     experiments, experiments_transferred,\n",
    "     database, database_transferred,\n",
    "     textmining, textmining_transferred,\n",
    "     initial_combined) = l\n",
    "\n",
    "\n",
    "    ## divide by 1000\n",
    "\n",
    "    neighborhood = float(neighborhood) / 1000\n",
    "    neighborhood_transferred = float(neighborhood_transferred) / 1000\n",
    "    fusion = float(fusion) / 1000\n",
    "    cooccurrence =  float(cooccurrence) / 1000\n",
    "    homology = float(homology) / 1000\n",
    "    coexpression = float(coexpression) / 1000\n",
    "    coexpression_transferred = float(coexpression_transferred) / 1000\n",
    "    experiments = float(experiments) / 1000\n",
    "    experiments_transferred = float(experiments_transferred) / 1000\n",
    "    database = float(database) / 1000\n",
    "    database_transferred = float(database_transferred) / 1000\n",
    "    textmining = float(textmining) / 1000\n",
    "    textmining_transferred = float(textmining_transferred) / 1000\n",
    "    initial_combined = int(initial_combined)\n",
    "\n",
    "\n",
    "    ## compute prior away\n",
    "\n",
    "    neighborhood_prior_corrected                 = compute_prior_away (neighborhood, prior)             \n",
    "    neighborhood_transferred_prior_corrected     = compute_prior_away (neighborhood_transferred, prior) \n",
    "    fusion_prior_corrected                       = compute_prior_away (fusion, prior)             \n",
    "    cooccurrence_prior_corrected                 = compute_prior_away (cooccurrence, prior)           \n",
    "    coexpression_prior_corrected                 = compute_prior_away (coexpression, prior)            \n",
    "    coexpression_transferred_prior_corrected     = compute_prior_away (coexpression_transferred, prior) \n",
    "    experiments_prior_corrected                  = compute_prior_away (experiments, prior)   \n",
    "    experiments_transferred_prior_corrected      = compute_prior_away (experiments_transferred, prior) \n",
    "    database_prior_corrected                     = compute_prior_away (database, prior)      \n",
    "    database_transferred_prior_corrected         = compute_prior_away (database_transferred, prior)\n",
    "    textmining_prior_corrected                   = compute_prior_away (textmining, prior)            \n",
    "    textmining_transferred_prior_corrected       = compute_prior_away (textmining_transferred, prior) \n",
    "\n",
    "    ## then, combine the direct and transferred scores for each category:\n",
    "\n",
    "    neighborhood_both_prior_corrected = 1.0 - (1.0 - neighborhood_prior_corrected) * (1.0 - neighborhood_transferred_prior_corrected)\n",
    "    coexpression_both_prior_corrected = 1.0 - (1.0 - coexpression_prior_corrected) * (1.0 - coexpression_transferred_prior_corrected)\n",
    "    experiments_both_prior_corrected = 1.0 - (1.0 - experiments_prior_corrected) * (1.0 - experiments_transferred_prior_corrected)\n",
    "    database_both_prior_corrected = 1.0 - (1.0 - database_prior_corrected) * (1.0 - database_transferred_prior_corrected)\n",
    "    textmining_both_prior_corrected = 1.0 - (1.0 - textmining_prior_corrected) * (1.0 - textmining_transferred_prior_corrected)\n",
    "\n",
    "    ## now, do the homology correction on cooccurrence and textmining:\n",
    "\n",
    "    cooccurrence_prior_homology_corrected = cooccurrence_prior_corrected * (1.0 - homology)\n",
    "    textmining_both_prior_homology_corrected = textmining_both_prior_corrected * (1.0 - homology)\n",
    "\n",
    "    ## next, do the 1 - multiplication:\n",
    "\n",
    "    combined_score_one_minus = (\n",
    "        (1.0 - neighborhood_both_prior_corrected) *\n",
    "        (1.0 - fusion_prior_corrected) *\n",
    "        (1.0 - cooccurrence_prior_homology_corrected) *\n",
    "        (1.0 - coexpression_both_prior_corrected) *\n",
    "        (1.0 - experiments_both_prior_corrected) *\n",
    "        (1.0 - database_both_prior_corrected) *\n",
    "        (1.0 - textmining_both_prior_homology_corrected) ) \n",
    "\n",
    "    ## and lastly, do the 1 - conversion again, and put back the prior *exactly once*\n",
    "\n",
    "    combined_score = (1.0 - combined_score_one_minus)            ## 1- conversion\n",
    "    combined_score *= (1.0 - prior)                              ## scale down\n",
    "    combined_score += prior                                      ## and add prior.\n",
    "\n",
    "    ## round\n",
    "\n",
    "    combined_score = int(combined_score * 1000)\n",
    "    print(protein1, protein2, combined_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
